{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ae167e-3c4b-4609-8e7c-7bf559a8fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "from models.linear_attn_sw import HybridAttention\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec346e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = 'YOUR_HF_TOKEN'\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc68a835-7956-48d1-b661-b1ee39fd8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf43ffac-a97f-451d-8a22-625f0bc67ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664f619-5174-44c2-b188-fd00cdef0fa9",
   "metadata": {},
   "source": [
    "## Replace with our trained hybrid blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec4bc8-1e72-4847-ba7a-c985c81c3242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced attention in layer 0 with hybrid attention\n",
      "Replaced attention in layer 2 with hybrid attention\n",
      "Replaced attention in layer 4 with hybrid attention\n",
      "Replaced attention in layer 6 with hybrid attention\n",
      "Replaced attention in layer 8 with hybrid attention\n",
      "Replaced attention in layer 10 with hybrid attention\n",
      "Replaced attention in layer 12 with hybrid attention\n",
      "Replaced attention in layer 14 with hybrid attention\n"
     ]
    }
   ],
   "source": [
    "layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "#layer_indices = [0]\n",
    "    \n",
    "# Load and replace each hybrid block\n",
    "for layer_idx in layer_indices:\n",
    "    # Create new hybrid attention\n",
    "    hybrid_attn = HybridAttention(model.config, layer_idx=layer_idx)\n",
    "    \n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(f\"hybrid_blocks/hybrid_layer_{layer_idx}.pt\")\n",
    "    hybrid_attn.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move to GPU\n",
    "    hybrid_attn = hybrid_attn.cuda()\n",
    "    \n",
    "    # Replace original attention with hybrid attention\n",
    "    model.model.layers[layer_idx].self_attn = hybrid_attn\n",
    "    \n",
    "    print(f\"Replaced attention in layer {layer_idx} with hybrid attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94239d-b8a2-4a95-b9e0-181bcb36d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"2+2=\"\n",
    "input_ids = tokenizer(text,return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245368b8-9dbd-42a8-916d-f602d12a8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=False,  # Disable KV cache\n",
    "    max_new_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1602fc1-7e9a-4b6b-9c66-040b128cefee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b024d914-de85-47e3-bd41-a087aa81d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+2= Brid brid brid brid brid brid brid brid Brid brid brid brid brid brid brid brid brid brid Bridge brid\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee37768-cc00-4b47-ab1f-5b0723428156",
   "metadata": {},
   "source": [
    "## Set up finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b008a2c7-4737-492f-8dde-af80f7ce26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_finetuning(model):\n",
    "    # First freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "    num_trainable_params = 0\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        hybrid_attn = model.model.layers[layer_idx].self_attn\n",
    "        \n",
    "        # 1. Unfreeze projection matrices\n",
    "        trainable_projections = [\n",
    "            hybrid_attn.q_proj.weight,\n",
    "            hybrid_attn.k_proj.weight,\n",
    "            hybrid_attn.v_proj.weight,\n",
    "            hybrid_attn.o_proj.weight\n",
    "        ]\n",
    "        for param in trainable_projections:\n",
    "            param.requires_grad = True\n",
    "            num_trainable_params += param.numel()\n",
    "        \n",
    "        # 2. Unfreeze mixing factors\n",
    "        hybrid_attn.window_factors.requires_grad = True\n",
    "        hybrid_attn.linear_factors.requires_grad = True\n",
    "        num_trainable_params += hybrid_attn.window_factors.numel()\n",
    "        num_trainable_params += hybrid_attn.linear_factors.numel()\n",
    "    \n",
    "    print(f\"Number of trainable parameters: {num_trainable_params:,}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a486da7d-5561-4b9b-a78f-57ce4e13041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_trainable_params(model):\n",
    "    print(\"\\nTrainable parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ff81e6-87c7-4576-aa39-cca4f9afe5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 83,886,592\n",
      "\n",
      "Trainable parameters:\n",
      "model.layers.0.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.0.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.0.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.2.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.2.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.4.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.4.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.6.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.6.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.8.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.8.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.10.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.10.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.12.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.12.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.14.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.14.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight: torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "model = prepare_for_finetuning(model)\n",
    "verify_trainable_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021c245-e090-4748-b98b-a3e108e81348",
   "metadata": {},
   "source": [
    "## Prepare Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc870ac-db23-44ae-a47e-795ad57adddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Set padding to left\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Format the prompt correctly based on the presence of the input field\n",
    "        if 'input' in item and item['input'].strip():\n",
    "            # Include the input if it's not empty\n",
    "            text = (\n",
    "                \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "                \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{item['instruction']}\\n\\n\"\n",
    "                f\"### Input:\\n{item['input']}\\n\\n\"\n",
    "                f\"### Response:\\n{item['output']}<|end of text|>\\n\\n\"\n",
    "            )\n",
    "        else:\n",
    "            # Exclude the input if it's not present or empty\n",
    "            text = (\n",
    "                \"Below is an instruction that describes a task. \"\n",
    "                \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{item['instruction']}\\n\\n\"\n",
    "                f\"### Response:\\n{item['output']}<|end of text|>\\n\\n\"\n",
    "            )\n",
    "\n",
    "        # Tokenize with truncation (but no padding at this stage)\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Create labels by setting padding tokens to -100 to be ignored in loss\n",
    "        labels = encodings['input_ids'].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82af755-efbd-434b-8f2e-e551b3dca586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def dynamic_collate_fn(batch, max_length=1024, pad_token_id=128001):  # Update pad_token_id to your tokenizer's eos token\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    current_batch, current_length = [], 0\n",
    "\n",
    "    for item in batch:\n",
    "        length = len(item['input_ids'])\n",
    "        \n",
    "        # Check if adding the sequence exceeds the max_length\n",
    "        if current_length + length > max_length:\n",
    "            # Pack the current batch into final tensors\n",
    "            packed_input_ids = torch.cat([x['input_ids'] for x in current_batch], dim=0)\n",
    "            packed_attention_mask = torch.cat([x['attention_mask'] for x in current_batch], dim=0)\n",
    "            packed_labels = torch.cat([x['labels'] for x in current_batch], dim=0)\n",
    "            \n",
    "            # Pad to max_length if needed\n",
    "            if len(packed_input_ids) < max_length:\n",
    "                padding_length = max_length - len(packed_input_ids)\n",
    "                packed_input_ids = torch.cat([\n",
    "                    packed_input_ids,\n",
    "                    torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "                ])\n",
    "                packed_attention_mask = torch.cat([\n",
    "                    packed_attention_mask,\n",
    "                    torch.zeros(padding_length, dtype=torch.long)\n",
    "                ])\n",
    "                packed_labels = torch.cat([\n",
    "                    packed_labels,\n",
    "                    torch.full((padding_length,), -100, dtype=torch.long)\n",
    "                ])\n",
    "            \n",
    "            # Append packed tensors\n",
    "            input_ids.append(packed_input_ids)\n",
    "            attention_masks.append(packed_attention_mask)\n",
    "            labels.append(packed_labels)\n",
    "            \n",
    "            # Reset for the next pack\n",
    "            current_batch, current_length = [], 0\n",
    "\n",
    "        # Add the current sequence to the batch\n",
    "        current_batch.append(item)\n",
    "        current_length += length\n",
    "\n",
    "    # Handle the last batch\n",
    "    if current_batch:\n",
    "        packed_input_ids = torch.cat([x['input_ids'] for x in current_batch], dim=0)\n",
    "        packed_attention_mask = torch.cat([x['attention_mask'] for x in current_batch], dim=0)\n",
    "        packed_labels = torch.cat([x['labels'] for x in current_batch], dim=0)\n",
    "        \n",
    "        # Pad the last batch if it doesn't fill max_length\n",
    "        if len(packed_input_ids) < max_length:\n",
    "            padding_length = max_length - len(packed_input_ids)\n",
    "            packed_input_ids = torch.cat([\n",
    "                packed_input_ids,\n",
    "                torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            packed_attention_mask = torch.cat([\n",
    "                packed_attention_mask,\n",
    "                torch.zeros(padding_length, dtype=torch.long)\n",
    "            ])\n",
    "            packed_labels = torch.cat([\n",
    "                packed_labels,\n",
    "                torch.full((padding_length,), -100, dtype=torch.long)\n",
    "            ])\n",
    "        \n",
    "        input_ids.append(packed_input_ids)\n",
    "        attention_masks.append(packed_attention_mask)\n",
    "        labels.append(packed_labels)\n",
    "\n",
    "    # Stack all batches into final tensors\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4fcbf3-3f12-49f7-9184-f24a3067543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset, val_dataset = train_val_split['train'], train_val_split['test']\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AlpacaDataset(train_dataset, tokenizer)\n",
    "val_dataset = AlpacaDataset(val_dataset, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,  # Adjust based on your GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=dynamic_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=dynamic_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcee9de-3dd6-4d5c-9c8b-5aed7cc078ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch shapes:\n",
      "input_ids shape: torch.Size([1, 1024])\n",
      "attention_mask shape: torch.Size([1, 1024])\n",
      "\n",
      "Decoded example:\n",
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Compare and contrast two characters in the movie The Avengers.\n",
      "\n",
      "### Input:\n",
      "Captain America and Iron Man\n",
      "\n",
      "### Response:\n",
      "Captain America and Iron Man are two of the main characters in the Marvel film, The Avengers. Both characters are members of the Avengers team and possess their own unique abilities and outlooks on life, but they have several key differences.\n",
      "\n",
      "Captain America, also known as Steve Rogers, is a super-soldier who gains his powers from a formula developed during World War II. He is the epitome of an honorable and moral hero, who puts the safety of others before his own. Captain America is seen as the defender of justice, and is always fighting for what is right, using his enhanced speed, strength, and durability to do so. He also values teamwork, leadership and discipline.\n",
      "\n",
      "Iron Man, on the other hand, is a genius inventor, also known as Tony Stark. He creates a suit of armor that gives him superhuman abilities including flight, enhanced strength, and an array of advanced weapons. Iron Man is known for his intellect, sarcasm, and ego. Unlike Captain America, he is often seen as a more selfish character, focusing on himself and his own desires, although with time he demonstrates love and care for his teammates and the world. \n",
      "\n",
      "The two characters often clash due to their differing views on how to approach situations. Captain America believes in following a strict moral code, while Iron Man is willing to take risks even if it means putting others in danger. Their personalities also differ, with Captain America being more reserved and serious, while Iron Man is more flamboyant and impulsive.\n",
      "\n",
      "In conclusion, both Captain America and Iron Man are important members of the Avengers team, each bringing their own unique abilities and personalities to the table. While they have several differences in their approach to being a hero, they ultimately work together to protect the world from harm.<|end of text|>\n",
      "\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain how to calculate the sum of the squares of all the numbers from 1 to 10.\n",
      "\n",
      "### Response:\n",
      "Calculating the sum of the squares of all the numbers from 1 to 10 involves adding the square of each number within the range. The formula for the sum of squares of the first n natural numbers is given by n(n+1)(2n+1)/6. Here's how you can do it:\n",
      "\n",
      "1. Start by listing out the first 10 natural numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
      "\n",
      "2. Next, square each of these numbers to get: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
      "\n",
      "3. Finally, add these numbers to get the sum of the squares of all the numbers from 1 to 10. The sum is 1 + 4 + 9 + 16 + 25 + 36 + 49 + 64 + 81 + 100 = 385.\n",
      "\n",
      "Alternatively, you can use the formula mentioned earlier to find the sum of the squares of the first 10 natural numbers:\n",
      "n(n+1)(2n+1)/6=10 (10+1)(2*10+1)/6= 10*11*21/6= 385.<|end of text|>\n",
      "\n",
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from train_loader\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# Print the shapes first\n",
    "print(\"\\nBatch shapes:\")\n",
    "print(f\"input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "\n",
    "# Decode a single example from the batch\n",
    "print(\"\\nDecoded example:\")\n",
    "single_example = sample_batch['input_ids'][0]  # Take first example from batch\n",
    "decoded_text = tokenizer.decode(single_example)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd7937-fa8b-4d0d-a448-82f7a7848461",
   "metadata": {},
   "source": [
    "## Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fbf2e4a-3c84-4f32-b1b4-66bd09c4c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",  \n",
    "        \"k_proj\",  \n",
    "        \"v_proj\",  \n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    modules_to_save=[\n",
    "        \"window_factors\",  \n",
    "        \"linear_factors\"   \n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2be7c5-5e78-4cbf-9faa-e0179a3d08d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b316376b-1a7a-4a55-8d96-ca9f8c369579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to PEFT\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7722c05c-3233-489a-89f1-d38ca4840a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1703936 || all params: 1237518848 || trainable%: 0.14\n"
     ]
    }
   ],
   "source": [
    "# Verify the trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_params} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_params:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06daaa2e-8caa-4920-bf69-8b8066c4230e",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79962fe-c89d-477b-aac2-4c89c7b36d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc58539-f961-4fa6-a308-966bd93270f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(model, train_loader, val_loader, save_dir=\"lora_checkpoints\", num_epochs=1):\n",
    "    \"\"\"LoRA finetuning with paper specifications\"\"\"\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Optimizer settings from paper\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    training_stats = {'train_losses': [], 'val_losses': []}\n",
    "    \n",
    "    print(f\"\\nStarting LoRA finetuning on {device}\")\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                          desc=f'LoRA Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for step, batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels'],\n",
    "                use_cache=False\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'avg_loss': f'{avg_loss:.3f}',\n",
    "                'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        training_stats['train_losses'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'],\n",
    "                    use_cache=False\n",
    "                )\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        training_stats['val_losses'].append(val_loss)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save if best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model.save_pretrained(save_dir / \"best_model\")\n",
    "            print(f\"Saved new best model! (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'training_stats': training_stats\n",
    "        }\n",
    "        torch.save(checkpoint, save_dir / f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35423fbc-8e31-4d8f-9254-a30c32a54154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting LoRA finetuning on cuda\n",
      "Number of training batches: 23292\n",
      "Number of validation batches: 2588\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ba145d70e4425afb86541cdd27dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 1/1:   0%|          | 0/23292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a398183c9a4701bb901b214d725391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Results:\n",
      "Training loss: 1.4209\n",
      "Validation loss: 1.3385\n",
      "Saved new best model! (val_loss: 1.3385)\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 1.3385\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"llama_lora_checkpoints\"\n",
    "model, stats = train_lora(model, train_loader, val_loader, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "605bc4b6-bd8e-4a71-8e51-7aab61914d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? How has the past week been for you? Have you been busy with work, family, or other\n"
     ]
    }
   ],
   "source": [
    "text = \"How are you?\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "outputs = model.generate(input_ids, max_new_tokens=20, use_cache=False)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2d5e8-1f8a-401d-9857-91618e0b6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
