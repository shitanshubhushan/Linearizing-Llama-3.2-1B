{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ae167e-3c4b-4609-8e7c-7bf559a8fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "from models.linear_attn_sw import HybridAttention\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9bda2d-925b-4a0a-b94f-5032b67b61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `598Project` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"YOUR_HF_TOKEN\"\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc68a835-7956-48d1-b661-b1ee39fd8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf43ffac-a97f-451d-8a22-625f0bc67ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664f619-5174-44c2-b188-fd00cdef0fa9",
   "metadata": {},
   "source": [
    "## Replace with our trained hybrid blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fec4bc8-1e72-4847-ba7a-c985c81c3242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced attention in layer 0 with hybrid attention\n",
      "Replaced attention in layer 2 with hybrid attention\n",
      "Replaced attention in layer 4 with hybrid attention\n",
      "Replaced attention in layer 6 with hybrid attention\n",
      "Replaced attention in layer 8 with hybrid attention\n",
      "Replaced attention in layer 10 with hybrid attention\n",
      "Replaced attention in layer 12 with hybrid attention\n",
      "Replaced attention in layer 14 with hybrid attention\n"
     ]
    }
   ],
   "source": [
    "layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "#layer_indices = [0]\n",
    "    \n",
    "# Load and replace each hybrid block\n",
    "for layer_idx in layer_indices:\n",
    "    # Create new hybrid attention\n",
    "    hybrid_attn = HybridAttention(model.config, layer_idx=layer_idx)\n",
    "    \n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(f\"hybrid_blocks/hybrid_layer_{layer_idx}.pt\")\n",
    "    hybrid_attn.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move to GPU\n",
    "    hybrid_attn = hybrid_attn.cuda()\n",
    "    \n",
    "    # Replace original attention with hybrid attention\n",
    "    model.model.layers[layer_idx].self_attn = hybrid_attn\n",
    "    \n",
    "    print(f\"Replaced attention in layer {layer_idx} with hybrid attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d94239d-b8a2-4a95-b9e0-181bcb36d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+2=elimelimelimelimelimelimelimelimelim proving Why meanings Why meaning Wol LETelimelimelimMakes\n"
     ]
    }
   ],
   "source": [
    "text = \"2+2=\"\n",
    "input_ids = tokenizer(text,return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=False,  # Disable KV cache\n",
    "    max_new_tokens=20\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee37768-cc00-4b47-ab1f-5b0723428156",
   "metadata": {},
   "source": [
    "## Set up finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b008a2c7-4737-492f-8dde-af80f7ce26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_finetuning(model):\n",
    "    # First freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "    num_trainable_params = 0\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        hybrid_attn = model.model.layers[layer_idx].self_attn\n",
    "        \n",
    "        # 1. Unfreeze projection matrices\n",
    "        trainable_projections = [\n",
    "            hybrid_attn.q_proj.weight,\n",
    "            hybrid_attn.k_proj.weight,\n",
    "            hybrid_attn.v_proj.weight,\n",
    "            hybrid_attn.o_proj.weight\n",
    "        ]\n",
    "        for param in trainable_projections:\n",
    "            param.requires_grad = True\n",
    "            num_trainable_params += param.numel()\n",
    "        \n",
    "        # 2. Unfreeze mixing factors\n",
    "        hybrid_attn.window_factors.requires_grad = True\n",
    "        hybrid_attn.linear_factors.requires_grad = True\n",
    "        num_trainable_params += hybrid_attn.window_factors.numel()\n",
    "        num_trainable_params += hybrid_attn.linear_factors.numel()\n",
    "    \n",
    "    print(f\"Number of trainable parameters: {num_trainable_params:,}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a486da7d-5561-4b9b-a78f-57ce4e13041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_trainable_params(model):\n",
    "    print(\"\\nTrainable parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ff81e6-87c7-4576-aa39-cca4f9afe5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 83,886,592\n",
      "\n",
      "Trainable parameters:\n",
      "model.layers.0.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.0.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.0.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.2.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.2.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.4.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.4.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.6.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.6.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.8.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.8.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.10.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.10.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.12.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.12.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.window_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.14.self_attn.linear_factors: torch.Size([1, 32, 1, 1])\n",
      "model.layers.14.self_attn.q_proj.weight: torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight: torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight: torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "model = prepare_for_finetuning(model)\n",
    "verify_trainable_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021c245-e090-4748-b98b-a3e108e81348",
   "metadata": {},
   "source": [
    "## Prepare Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc870ac-db23-44ae-a47e-795ad57adddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DollyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=1024):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Set padding to left\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Basic prompt structure\n",
    "        prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "        prompt += f\"### Instruction:\\n{item['instruction']}\\n\\n\"\n",
    "        \n",
    "        # Add context only if it exists and is not empty/null\n",
    "        if 'context' in item and item['context'] and not isinstance(item['context'], float):  # Check for NaN\n",
    "            context = item['context'].strip()\n",
    "            if context:\n",
    "                prompt += f\"### Context:\\n{context}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"### Response:\\n{item['response']}<|end of text|>\\n\\n\"\n",
    "\n",
    "        # Tokenize with truncation\n",
    "        encodings = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Create labels by setting padding tokens to -100 to be ignored in loss\n",
    "        labels = encodings['input_ids'].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82af755-efbd-434b-8f2e-e551b3dca586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def dynamic_collate_fn(batch, max_length=1024, pad_token_id=128001):  # Update pad_token_id to your tokenizer's eos token\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    current_batch, current_length = [], 0\n",
    "\n",
    "    for item in batch:\n",
    "        length = len(item['input_ids'])\n",
    "        \n",
    "        # Check if adding the sequence exceeds the max_length\n",
    "        if current_length + length > max_length:\n",
    "            # Pack the current batch into final tensors\n",
    "            packed_input_ids = torch.cat([x['input_ids'] for x in current_batch], dim=0)\n",
    "            packed_attention_mask = torch.cat([x['attention_mask'] for x in current_batch], dim=0)\n",
    "            packed_labels = torch.cat([x['labels'] for x in current_batch], dim=0)\n",
    "            \n",
    "            # Pad to max_length if needed\n",
    "            if len(packed_input_ids) < max_length:\n",
    "                padding_length = max_length - len(packed_input_ids)\n",
    "                packed_input_ids = torch.cat([\n",
    "                    packed_input_ids,\n",
    "                    torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "                ])\n",
    "                packed_attention_mask = torch.cat([\n",
    "                    packed_attention_mask,\n",
    "                    torch.zeros(padding_length, dtype=torch.long)\n",
    "                ])\n",
    "                packed_labels = torch.cat([\n",
    "                    packed_labels,\n",
    "                    torch.full((padding_length,), -100, dtype=torch.long)\n",
    "                ])\n",
    "            \n",
    "            # Append packed tensors\n",
    "            input_ids.append(packed_input_ids)\n",
    "            attention_masks.append(packed_attention_mask)\n",
    "            labels.append(packed_labels)\n",
    "            \n",
    "            # Reset for the next pack\n",
    "            current_batch, current_length = [], 0\n",
    "\n",
    "        # Add the current sequence to the batch\n",
    "        current_batch.append(item)\n",
    "        current_length += length\n",
    "\n",
    "    # Handle the last batch\n",
    "    if current_batch:\n",
    "        packed_input_ids = torch.cat([x['input_ids'] for x in current_batch], dim=0)\n",
    "        packed_attention_mask = torch.cat([x['attention_mask'] for x in current_batch], dim=0)\n",
    "        packed_labels = torch.cat([x['labels'] for x in current_batch], dim=0)\n",
    "        \n",
    "        # Pad the last batch if it doesn't fill max_length\n",
    "        if len(packed_input_ids) < max_length:\n",
    "            padding_length = max_length - len(packed_input_ids)\n",
    "            packed_input_ids = torch.cat([\n",
    "                packed_input_ids,\n",
    "                torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            packed_attention_mask = torch.cat([\n",
    "                packed_attention_mask,\n",
    "                torch.zeros(padding_length, dtype=torch.long)\n",
    "            ])\n",
    "            packed_labels = torch.cat([\n",
    "                packed_labels,\n",
    "                torch.full((padding_length,), -100, dtype=torch.long)\n",
    "            ])\n",
    "        \n",
    "        input_ids.append(packed_input_ids)\n",
    "        attention_masks.append(packed_attention_mask)\n",
    "        labels.append(packed_labels)\n",
    "\n",
    "    # Stack all batches into final tensors\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4fcbf3-3f12-49f7-9184-f24a3067543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dolly-15k dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Dolly-15k dataset...\")\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "train_val_split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset, val_dataset = train_val_split['train'], train_val_split['test']\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DollyDataset(train_dataset, tokenizer)\n",
    "val_dataset = DollyDataset(val_dataset, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,  # Adjust based on your GPU memory\n",
    "    shuffle=True,\n",
    "    collate_fn=dynamic_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=dynamic_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb091cb6-8ac5-4e70-b098-4652ee596ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing dataloader with a sample batch...\n",
      "\n",
      "Batch shapes:\n",
      "input_ids shape: torch.Size([1, 1024])\n",
      "attention_mask shape: torch.Size([1, 1024])\n",
      "\n",
      "Decoded example:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an Edgeworth box in Economics?\n",
      "\n",
      "### Response:\n",
      "An Edgeworth box in Economics, is a graphical representation of a market with just two commodities, X and Y, and two consumers. The dimensions of the box are the total quantities Ωx and Ωy of the two goods.<|end of text|>\n",
      "\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is a Balance sheet?\n",
      "\n",
      "### Response:\n",
      "A balance sheet is a summary of an organisation's financial position. It lists the values, in the books of account on a particular date, of all the organisation's assets and liabilities. The assets and liabilities are grouped in categories, to paint a picture of the organisation's strengths and weaknesses.<|end of text|>\n",
      "\n",
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "print(\"\\nTesting dataloader with a sample batch...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "print(f\"input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "\n",
    "# Print an example to verify formatting\n",
    "print(\"\\nDecoded example:\")\n",
    "single_example = sample_batch['input_ids'][0]\n",
    "decoded_text = tokenizer.decode(single_example)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd7937-fa8b-4d0d-a448-82f7a7848461",
   "metadata": {},
   "source": [
    "## Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fbf2e4a-3c84-4f32-b1b4-66bd09c4c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",  \n",
    "        \"k_proj\",  \n",
    "        \"v_proj\",  \n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    modules_to_save=[\n",
    "        \"window_factors\",  \n",
    "        \"linear_factors\"   \n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b316376b-1a7a-4a55-8d96-ca9f8c369579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to PEFT\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7722c05c-3233-489a-89f1-d38ca4840a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1703936 || all params: 1237518848 || trainable%: 0.14\n"
     ]
    }
   ],
   "source": [
    "# Verify the trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_params} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_params:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06daaa2e-8caa-4920-bf69-8b8066c4230e",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79962fe-c89d-477b-aac2-4c89c7b36d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc58539-f961-4fa6-a308-966bd93270f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(model, train_loader, val_loader, save_dir=\"lora_checkpoints\", num_epochs=1):\n",
    "    \"\"\"LoRA finetuning with paper specifications\"\"\"\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Optimizer settings from paper\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    training_stats = {'train_losses': [], 'val_losses': []}\n",
    "    \n",
    "    print(f\"\\nStarting LoRA finetuning on {device}\")\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                          desc=f'LoRA Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for step, batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels'],\n",
    "                use_cache=False\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'avg_loss': f'{avg_loss:.3f}',\n",
    "                'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        training_stats['train_losses'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'],\n",
    "                    use_cache=False\n",
    "                )\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        training_stats['val_losses'].append(val_loss)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save if best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model.save_pretrained(save_dir / \"best_model\")\n",
    "            print(f\"Saved new best model! (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'training_stats': training_stats\n",
    "        }\n",
    "        torch.save(checkpoint, save_dir / f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35423fbc-8e31-4d8f-9254-a30c32a54154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting LoRA finetuning on cuda\n",
      "Number of training batches: 6755\n",
      "Number of validation batches: 751\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27e9f4e94ec4348b347a358e7d8dd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 1/1:   0%|          | 0/6755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7699d39551cc463ba82697e8c77beb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/751 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Results:\n",
      "Training loss: 2.0905\n",
      "Validation loss: 1.9463\n",
      "Saved new best model! (val_loss: 1.9463)\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 1.9463\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"llama_lora_checkpoints\"\n",
    "model, stats = train_lora(model, train_loader, val_loader, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "605bc4b6-bd8e-4a71-8e51-7aab61914d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+2=4\n",
      "3+3=6\n",
      "4+4=8\n",
      "5+5=10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"2+2=\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "outputs = model.generate(input_ids, max_new_tokens=20, use_cache=False)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2d5e8-1f8a-401d-9857-91618e0b6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
