{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d74b022-4818-4508-9350-03cb4decb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "!pip install transformers datasets tqdm numpy torch\n",
    "!pip install mambapy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460ba515-b8f7-4b80-a507-15b10df16e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141e7e4f-70a2-4666-ab2c-c7aadba10d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `598Project` has been saved to /home/sbhushan/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/sbhushan/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc4e058-0ce7-4458-8c80-b7c4b57db80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 - Tesla V100-PCIE-16GB\n",
      "Total Memory: 15.77 GB\n",
      "Allocated Memory: 0.00 GB\n",
      "Cached Memory: 0.00 GB\n",
      "Free Memory: 15.77 GB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"GPU {i} - {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "            print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "            print(f\"Free Memory: {free_memory:.2f} GB\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "# Call the function\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc20388-5b88-4d44-a4f8-bb335372beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_check(step=\"\"):\n",
    "    print(f\"\\nMemory Check - {step}\")\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e670aac6-8ad3-4648-ae1d-7f7dbc45898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from mambapy.mamba import Mamba, MambaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90776912-ab6d-4075-8ab8-dc7f0c9a37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8039dac0-dedc-4787-8be2-6ab53df547f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba218c1eb064c899a0d061e95de15a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load streaming dataset\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
    "                      name=\"sample-10BT\", \n",
    "                      split=\"train\", \n",
    "                      streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870d1fff-06c4-479b-b9d5-7a5dc3b57636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 101,374 tokens\n",
      "Collected 201,135 tokens\n",
      "Collected 301,190 tokens\n",
      "Collected 400,573 tokens\n",
      "Collected 500,666 tokens\n",
      "Collected 600,893 tokens\n",
      "Collected 701,230 tokens\n",
      "Collected 800,062 tokens\n",
      "Collected 900,056 tokens\n",
      "Collected 1,000,303 tokens\n",
      "Collected 1,104,261 tokens\n",
      "Collected 1,200,768 tokens\n",
      "Collected 1,300,278 tokens\n",
      "Collected 1,400,226 tokens\n",
      "Collected 1,500,380 tokens\n",
      "Collected 1,600,015 tokens\n",
      "Collected 1,700,608 tokens\n",
      "Collected 1,810,446 tokens\n",
      "Collected 1,902,046 tokens\n",
      "Collected 2,002,909 tokens\n",
      "Collected 2,100,052 tokens\n",
      "Collected 2,200,058 tokens\n",
      "Collected 2,303,549 tokens\n",
      "Collected 2,401,422 tokens\n",
      "Collected 2,500,434 tokens\n",
      "Collected 2,600,557 tokens\n",
      "Collected 2,700,392 tokens\n",
      "Collected 2,801,133 tokens\n",
      "Collected 2,900,204 tokens\n",
      "Collected 3,001,947 tokens\n",
      "Collected 3,101,877 tokens\n",
      "Collected 3,200,123 tokens\n",
      "Collected 3,300,891 tokens\n",
      "Collected 3,400,373 tokens\n",
      "Collected 3,500,236 tokens\n",
      "Collected 3,600,082 tokens\n",
      "Collected 3,700,818 tokens\n",
      "Collected 3,802,260 tokens\n",
      "Collected 3,900,198 tokens\n",
      "Collected 4,000,067 tokens\n",
      "Collected 4,107,489 tokens\n",
      "Collected 4,201,785 tokens\n",
      "Collected 4,300,823 tokens\n",
      "Collected 4,408,609 tokens\n",
      "Collected 4,501,101 tokens\n",
      "Collected 4,600,302 tokens\n",
      "Collected 4,703,247 tokens\n",
      "Collected 4,800,100 tokens\n",
      "Collected 4,900,069 tokens\n",
      "Collected 5,002,422 tokens\n",
      "\n",
      "Final token count: 5,000,000\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage\n",
    "collected_tokens = []\n",
    "total_tokens = 0\n",
    "target_tokens = 5_000_000\n",
    "\n",
    "# Collect samples\n",
    "for sample in dataset:\n",
    "    # Get text from sample\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(text, truncation=False, padding=False)['input_ids']\n",
    "    \n",
    "    # Add to collection\n",
    "    collected_tokens.extend(tokens)\n",
    "    total_tokens += len(tokens)\n",
    "    \n",
    "    # Print progress every 100k tokens\n",
    "    if total_tokens // 100_000 > (total_tokens - len(tokens)) // 100_000:\n",
    "        print(f\"Collected {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Stop when we hit target\n",
    "    if total_tokens >= target_tokens:\n",
    "        break\n",
    "\n",
    "# Convert to numpy array and trim to exact size\n",
    "collected_tokens = np.array(collected_tokens[:target_tokens])\n",
    "print(f\"\\nFinal token count: {len(collected_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216b90ed-e11b-426d-965e-50c5b1a86e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9765, 511])\n",
      "Target shape: torch.Size([9765, 511])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 512  # Changed from 1024\n",
    "n_sequences = len(collected_tokens) // sequence_length\n",
    "\n",
    "# Reshape tokens into sequences\n",
    "sequences = collected_tokens[:n_sequences * sequence_length].reshape(-1, sequence_length)\n",
    "\n",
    "# Create input and target pairs for causal language modeling\n",
    "input_sequences = sequences[:, :-1]  # all tokens except last\n",
    "target_sequences = sequences[:, 1:]  # all tokens except first\n",
    "\n",
    "# Convert to torch tensors\n",
    "inputs = torch.tensor(input_sequences)\n",
    "masks = torch.tensor(np.ones_like(input_sequences))\n",
    "targets = torch.tensor(target_sequences)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788a9148-64aa-40cb-ab49-a15b23411da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "val_idx = int(len(inputs) * (1 - val_split))\n",
    "\n",
    "# Split into train/val\n",
    "train_inputs = inputs[:val_idx]\n",
    "train_masks = masks[:val_idx]\n",
    "train_targets = targets[:val_idx]\n",
    "\n",
    "val_inputs = inputs[val_idx:]\n",
    "val_masks = masks[val_idx:]\n",
    "val_targets = targets[val_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LLMDataset(train_inputs, train_masks, train_targets)\n",
    "val_dataset = LLMDataset(val_inputs, val_masks, val_targets)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,  # Changed to 4\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0cad1cb-6c60-4d2a-bef2-cbb5c93129c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Training batches: 4394\n",
      "Validation batches: 489\n",
      "\n",
      "Batch shapes:\n",
      "input_ids: torch.Size([2, 511])\n",
      "attention_mask: torch.Size([2, 511])\n",
      "labels: torch.Size([2, 511])\n",
      "\n",
      "Sample decoded text:\n",
      " Betty LaRue was joined by the New York\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Sample a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "# Decode a sample sequence\n",
    "sample_seq = sample_batch['input_ids'][0][:10].tolist()\n",
    "decoded = tokenizer.decode(sample_seq)\n",
    "print(f\"\\nSample decoded text:\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de1815f-6240-4884-b146-6d3b96fd1371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "#model = model.half()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5d6348-0c56-495f-bfbc-64e4f95a7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 0 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 2 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 2 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 4 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 4 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 6 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 6 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 8 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 8 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 10 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 10 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 12 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 12 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 14 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 14 - Input dtype: torch.float32, Output dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Initialize 8 Mamba blocks\n",
    "mamba_blocks = {}\n",
    "\n",
    "# Configuration for all blocks\n",
    "mamba_config = MambaConfig(\n",
    "    d_model=2048,          # Matches Llama's hidden size\n",
    "    n_layers=1             \n",
    ")\n",
    "\n",
    "# Create blocks for layers 0,2,4,6,8,10,12,14\n",
    "#for layer_idx in [1, 3, 5, 7, 8, 10, 12, 14]:\n",
    "for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "    \n",
    "    mamba_blocks[layer_idx] = Mamba(mamba_config).cuda()\n",
    "\n",
    "# Verify shapes for each block\n",
    "test_input = torch.randn(1, 5, 2048).cuda()  # Changed dtype\n",
    "for layer_idx, block in mamba_blocks.items():\n",
    "    test_output = block(test_input)\n",
    "    print(f\"Layer {layer_idx} - Input shape: {test_input.shape}, Output shape: {test_output.shape}\")\n",
    "    print(f\"Layer {layer_idx} - Input dtype: {test_input.dtype}, Output dtype: {test_output.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e08e220-c657-4d36-9735-84c305e33547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA trainable parameters: 0 (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Freeze LLaMA\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify LLaMA is frozen\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"LLaMA trainable parameters: {len(trainable_params)} (should be 0)\")\n",
    "\n",
    "# Ensure Mamba blocks are trainable\n",
    "for mamba in mamba_blocks.values():\n",
    "    for param in mamba.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d68dae-6a29-4d39-95c2-f14c380e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {}\n",
    "for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "    optimizers[layer_idx] = torch.optim.AdamW(mamba_blocks[layer_idx].parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14675a87-0ca2-4b56-8d37-c1dd9e4b1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5858f92c-5f08-4908-9cdd-c0a4ed046b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, mamba_blocks, train_loader, optimizers, accumulation_steps, mse_factor=1e3):\n",
    "    layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "    layer_losses = {idx: 0.0 for idx in layer_indices}\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Training')\n",
    "    \n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # Move inputs to GPU efficiently\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        \n",
    "        # Create position IDs\n",
    "        input_shape = input_ids.size()\n",
    "        position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get LLaMA hidden states and cache them\n",
    "        with torch.no_grad():\n",
    "            true_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                position_ids=position_ids,  # Add position_ids here\n",
    "                output_hidden_states=True,\n",
    "                use_cache=False\n",
    "            )\n",
    "            hidden_states = true_outputs.hidden_states\n",
    "            \n",
    "            # Extract teacher model outputs for each layer\n",
    "            teacher_outputs = []\n",
    "            for layer_idx in layer_indices:\n",
    "                layer = model.model.layers[layer_idx]\n",
    "                # Get normalized input and attention output\n",
    "                layer_input = layer.input_layernorm(hidden_states[layer_idx])\n",
    "                # Call attention with hidden states and position_ids\n",
    "                attn_output = layer.self_attn(\n",
    "                    hidden_states=layer_input,\n",
    "                    position_ids=position_ids  # Add position_ids here\n",
    "                )[0]\n",
    "                \n",
    "                # Verify shapes before storing\n",
    "                if attn_output is not None and layer_input is not None:\n",
    "                    teacher_outputs.append((layer_input.cpu(), attn_output.cpu()))\n",
    "                else:\n",
    "                    raise ValueError(f\"Layer {layer_idx} produced None output\")\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            del true_outputs, hidden_states\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Process each Mamba block\n",
    "        total_loss = 0\n",
    "        for idx, layer_idx in enumerate(layer_indices):\n",
    "            layer_input, teacher_output = teacher_outputs[idx]\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizers[layer_idx].zero_grad()\n",
    "            \n",
    "            # Forward pass through Mamba\n",
    "            mamba_output = mamba_blocks[layer_idx](\n",
    "                layer_input.cuda(),\n",
    "            )\n",
    "            \n",
    "            # Verify Mamba output shape\n",
    "            if mamba_output is None:\n",
    "                raise ValueError(f\"Mamba block {layer_idx} produced None output\")\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(\n",
    "                mamba_output,\n",
    "                teacher_output.cuda()\n",
    "            ) * mse_factor\n",
    "            \n",
    "            # Scale loss and backward pass\n",
    "            scaled_loss = loss / accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "            # Update tracking\n",
    "            layer_losses[layer_idx] += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Optimizer step on accumulation boundary\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                optimizers[layer_idx].step()\n",
    "            \n",
    "            # Clear memory\n",
    "            del mamba_output, layer_input, teacher_output\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            avg_losses = {\n",
    "                idx: loss/accumulation_steps \n",
    "                for idx, loss in layer_losses.items()\n",
    "            }\n",
    "            avg_total = total_loss/len(layer_indices)\n",
    "            \n",
    "            loss_str = \" \".join([\n",
    "                f\"L{idx}: {loss:.4f}\" \n",
    "                for idx, loss in avg_losses.items()\n",
    "            ])\n",
    "            progress_bar.set_description(\n",
    "                f\"Train | Avg: {avg_total:.4f} | {loss_str}\"\n",
    "            )\n",
    "            \n",
    "            # Reset loss tracking\n",
    "            layer_losses = {idx: 0.0 for idx in layer_indices}\n",
    "            total_loss = 0\n",
    "            \n",
    "    return layer_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b763c9e9-f202-4174-a31e-74c9b1060a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, mamba_blocks, val_loader, mse_factor=1e3):\n",
    "    layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "    layer_losses = {idx: 0.0 for idx in layer_indices}\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            position_ids = torch.arange(0, input_ids.size(-1), device=input_ids.device).unsqueeze(0)\n",
    "            \n",
    "            # Get teacher outputs\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          position_ids=position_ids,\n",
    "                          output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # Get teacher attention outputs\n",
    "            teacher_outputs = []\n",
    "            for layer_idx in layer_indices:\n",
    "                layer = model.model.layers[layer_idx]\n",
    "                layer_input = layer.input_layernorm(hidden_states[layer_idx])\n",
    "                attn_output = layer.self_attn(\n",
    "                    hidden_states=layer_input,\n",
    "                    position_ids=position_ids\n",
    "                )[0]\n",
    "                teacher_outputs.append((layer_input, attn_output))\n",
    "            \n",
    "            # Compare with Mamba outputs\n",
    "            for idx, layer_idx in enumerate(layer_indices):\n",
    "                layer_input, teacher_output = teacher_outputs[idx]\n",
    "                mamba_output = mamba_blocks[layer_idx](layer_input)\n",
    "                loss = F.mse_loss(mamba_output, teacher_output) * mse_factor\n",
    "                layer_losses[layer_idx] += loss.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "    return {idx: loss/num_batches for idx, loss in layer_losses.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e99ef9-c6f5-4b42-a4d7-09f02f73b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('mamba_blocks', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae654c14-50aa-4629-b769-b632899c8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c143e4faafe4f88a3ad1b549992b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation losses: L0: 10.9501 L2: 62.4131 L4: 64.3233 L6: 79.8924 L8: 91.3085 L10: 116.2376 L12: 97.6159 L14: 106.8220\n",
      "All layers converged - stopping training\n",
      "\n",
      "Training completed in: 0:59:47\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "accumulation_steps = 32\n",
    "best_val_losses = {idx: float('inf') for idx in [0, 2, 4, 6, 8, 10, 12, 14]}\n",
    "patience = 3\n",
    "no_improve_count = {idx: 0 for idx in [0, 2, 4, 6, 8, 10, 12, 14]}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_losses = train_epoch(model, mamba_blocks, train_loader, optimizers, accumulation_steps)\n",
    "    \n",
    "    # Validate\n",
    "    val_losses = validate(model, mamba_blocks, val_loader)\n",
    "    \n",
    "    # Print validation losses\n",
    "    val_str = \" \".join([f\"L{idx}: {loss:.4f}\" for idx, loss in val_losses.items()])\n",
    "    print(f\"Validation losses: {val_str}\")\n",
    "    \n",
    "    # Check early stopping per layer\n",
    "    active_layers = False\n",
    "    for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "        if val_losses[layer_idx] < best_val_losses[layer_idx]:\n",
    "            best_val_losses[layer_idx] = val_losses[layer_idx]\n",
    "            no_improve_count[layer_idx] = 0\n",
    "            # Save best model for this layer\n",
    "            torch.save(mamba_blocks[layer_idx].state_dict(), f'mamba_blocks/mamba_layer_{layer_idx}.pt')\n",
    "        else:\n",
    "            no_improve_count[layer_idx] += 1\n",
    "            if no_improve_count[layer_idx] >= patience:\n",
    "                print(f\"Early stopping for layer {layer_idx}\")\n",
    "                # Load best model for this layer\n",
    "                mamba_blocks[layer_idx].load_state_dict(torch.load(f'mamba_blocks/mamba_layer_{layer_idx}.pt'))\n",
    "            else:\n",
    "                active_layers = True\n",
    "    \n",
    "    if not active_layers:\n",
    "        print(\"All layers converged - stopping training\")\n",
    "        break\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Time elapsed: {timedelta(seconds=int(epoch_time))}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in: {timedelta(seconds=int(total_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cc7ce69-30a7-4c24-ba94-f156a90c6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaWrapper(torch.nn.Module):\n",
    "    def __init__(self, mamba_block):\n",
    "        super().__init__()\n",
    "        self.mamba = mamba_block\n",
    "        self.last_state = None\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, position_ids=None, past_key_value=None, output_attentions=False, use_cache=False, **kwargs):\n",
    "        if past_key_value is not None:\n",
    "            self.last_state = past_key_value\n",
    "        output = self.mamba(hidden_states)\n",
    "        self.last_state = output.last_state if hasattr(output, 'last_state') else None\n",
    "        return (output, None, self.last_state)\n",
    "\n",
    "# Replace attention with wrapped Mamba blocks\n",
    "for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "    model.model.layers[layer_idx].self_attn = MambaWrapper(mamba_blocks[layer_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e872530-25dc-4451-956e-bc9cb553968a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# batch_size=1, seq_len=10\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(test_input)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward pass successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_input = torch.randint(0, 1000, (1, 10)).cuda()  # batch_size=1, seq_len=10\n",
    "test_output = model(test_input)\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "281ff2c0-7d15-42e7-88b2-5d5733a51643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_finetuning(model):\n",
    "    print(\"Freezing all parameters except Mamba blocks...\")\n",
    "    \n",
    "    # First freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Count trainable parameters and unfreeze only Mamba blocks\n",
    "    num_params = 0\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if isinstance(layer.self_attn, MambaWrapper):\n",
    "            for param in layer.self_attn.mamba.parameters():\n",
    "                param.requires_grad = True\n",
    "                num_params += param.numel()\n",
    "    \n",
    "    print(f\"Number of trainable parameters: {num_params:,}\")\n",
    "\n",
    "    # Verify which layers are trainable\n",
    "    print(\"\\nTrainable layers:\")\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if isinstance(layer.self_attn, MambaWrapper):\n",
    "            num_trainable = sum(p.requires_grad for p in layer.self_attn.mamba.parameters())\n",
    "            print(f\"Layer {layer_idx}: {num_trainable} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e19f5855-d0b3-4c46-b045-8a431ce5a2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prepare_for_finetuning(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "prepare_for_finetuning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "128046b6-52e5-4290-8e03-2f0446cd3380",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m verify_grad_status(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def verify_grad_status(model):\n",
    "    print(\"\\nGradient status check:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Trainable: {name}\")\n",
    "\n",
    "verify_grad_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86186f64-f252-4af8-b956-5665a3849f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHybrid model saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the current model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m save_hybrid_model(\u001b[43mmodel\u001b[49m, mamba_blocks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def save_hybrid_model(model, mamba_blocks):\n",
    "    save_dict = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'mamba_configs': {\n",
    "            'hidden_size': 2048,\n",
    "            'replaced_layers': [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "        },\n",
    "        'mamba_states': {\n",
    "            idx: block.state_dict() \n",
    "            for idx, block in mamba_blocks.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs('hybrid_model', exist_ok=True)\n",
    "    torch.save(save_dict, 'hybrid_model/pre_lora_model.pt')\n",
    "    print(\"Hybrid model saved successfully!\")\n",
    "\n",
    "# Save the current model\n",
    "save_hybrid_model(model, mamba_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f63006a4-40a4-4ceb-abd8-e811fd417fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 46584\n",
      "Validation samples: 5176\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "train_val = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset, val_dataset = train_val['train'], train_val['test']\n",
    "\n",
    "# Format with template\n",
    "def format_alpaca_prompt(instruction, output):\n",
    "    return f\"<|begin of text|> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}<|end of text|>\"\n",
    "\n",
    "# Create a custom dataset class\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = format_alpaca_prompt(item['instruction'], item['output'])\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# Create proper collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AlpacaDataset(train_val['train'], tokenizer)\n",
    "val_dataset = AlpacaDataset(train_val['test'], tokenizer)\n",
    "\n",
    "# Create dataloaders with collate_fn\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a7356c3-03b6-46c2-81b7-e6f79e406bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete existing model and optimizers if they exist\n",
    "try:\n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58517af3-57f2-4ad7-908f-8a96bcbbb22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Memory Status:\n",
      "Total Memory: 16.93 GB\n",
      "Allocated Memory: 15.99 GB\n",
      "Cached Memory: 16.07 GB\n",
      "Free Memory: 0.94 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    print(\"\\nGPU Memory Status:\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "    print(f\"Allocated Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Cached Memory: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "    print(f\"Free Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1e9:.2f} GB\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55f226be-5b70-4cbd-941d-8843a43dbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA config targeting Mamba projection layers\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"in_proj\", \"x_proj\", \"dt_proj\", \"out_proj\"],  # Mamba projection layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff2f258-65c4-446c-b81c-733b62a6d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_lora(model, train_loader, optimizer, scheduler, accumulation_steps=128):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    \n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / accumulation_steps\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % (accumulation_steps * 4) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        progress_bar.set_description(\n",
    "            f\"Loss: {total_loss/(batch_idx+1):.4f}\"\n",
    "        )\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ad44151-e49a-442b-b1f0-501e95489b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lora(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfc1f45a-793d-4f9b-b673-a729663911ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Number of training batches: 23292\n",
      "Number of validation batches: 2588\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2491170be2e4659a104ad47cf37927e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 172.19 MiB is free. Including non-PyTorch memory, this process has 15.59 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 78.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Clear cache at start of epoch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 28\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_lora(model, val_loader)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m, in \u001b[0;36mtrain_epoch_lora\u001b[0;34m(model, train_loader, optimizer, scheduler, accumulation_steps)\u001b[0m\n\u001b[1;32m      8\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36mMambaWrapper.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_state \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_state \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_state\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (output, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:83\u001b[0m, in \u001b[0;36mMamba.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m#x : (B, L, D)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#y : (B, L, D)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 83\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:111\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m#x : (B, L, D)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m#output : (B, L, D)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:222\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#(B, L, ED)\u001b[39;00m\n\u001b[1;32m    221\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(x)\n\u001b[0;32m--> 222\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[1;32m    225\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(y) \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:268\u001b[0m, in \u001b[0;36mMambaBlock.ssm\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m    265\u001b[0m delta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(delta \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_proj\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpscan:\n\u001b[0;32m--> 268\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselective_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselective_scan_seq(x, delta, A, B, C, D)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:284\u001b[0m, in \u001b[0;36mMambaBlock.selective_scan\u001b[0;34m(self, x, delta, A, B, C, D)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselective_scan\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, delta, A, B, C, D):\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m#x : (B, L, ED)\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# : (B, L, ED)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m#y : (B, L, ED)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     deltaA \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(B, L, ED, N)\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     deltaB \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m B\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#(B, L, ED, N)\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     BX \u001b[38;5;241m=\u001b[39m deltaB \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#(B, L, ED, N)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 172.19 MiB is free. Including non-PyTorch memory, this process has 15.59 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 78.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=len(train_loader)\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "no_improve = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Clear cache at start of epoch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_loss = train_epoch_lora(\n",
    "        model, \n",
    "        train_loader, \n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    val_loss = validate_lora(model, val_loader)\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        print(\"Saving best model...\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, \"best_hybrid_model.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a6e22-45b2-43b3-86d2-3e938485a524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
