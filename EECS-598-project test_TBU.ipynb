{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d74b022-4818-4508-9350-03cb4decb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "!pip install transformers datasets tqdm numpy torch\n",
    "!pip install mambapy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141e7e4f-70a2-4666-ab2c-c7aadba10d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `598Project` has been saved to /home/sbhushan/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/sbhushan/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Should get access to llama 3.2 1B\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = 'ENTER YOUR TOKEN'\n",
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc4e058-0ce7-4458-8c80-b7c4b57db80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 - Tesla V100-PCIE-16GB\n",
      "Total Memory: 15.77 GB\n",
      "Allocated Memory: 0.00 GB\n",
      "Cached Memory: 0.00 GB\n",
      "Free Memory: 15.77 GB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"GPU {i} - {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "            print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
    "            print(f\"Free Memory: {free_memory:.2f} GB\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "# Call the function\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc20388-5b88-4d44-a4f8-bb335372beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_check(step=\"\"):\n",
    "    print(f\"\\nMemory Check - {step}\")\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e670aac6-8ad3-4648-ae1d-7f7dbc45898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from mambapy.mamba import Mamba, MambaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90776912-ab6d-4075-8ab8-dc7f0c9a37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8039dac0-dedc-4787-8be2-6ab53df547f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e969366fa264d00801703af979e4fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load streaming dataset\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
    "                      name=\"sample-10BT\", \n",
    "                      split=\"train\", \n",
    "                      streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870d1fff-06c4-479b-b9d5-7a5dc3b57636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 101,374 tokens\n",
      "Collected 201,135 tokens\n",
      "Collected 301,190 tokens\n",
      "Collected 400,573 tokens\n",
      "Collected 500,666 tokens\n",
      "Collected 600,893 tokens\n",
      "Collected 701,230 tokens\n",
      "Collected 800,062 tokens\n",
      "Collected 900,056 tokens\n",
      "Collected 1,000,303 tokens\n",
      "Collected 1,104,261 tokens\n",
      "Collected 1,200,768 tokens\n",
      "Collected 1,300,278 tokens\n",
      "Collected 1,400,226 tokens\n",
      "Collected 1,500,380 tokens\n",
      "Collected 1,600,015 tokens\n",
      "Collected 1,700,608 tokens\n",
      "Collected 1,810,446 tokens\n",
      "Collected 1,902,046 tokens\n",
      "Collected 2,002,909 tokens\n",
      "Collected 2,100,052 tokens\n",
      "Collected 2,200,058 tokens\n",
      "Collected 2,303,549 tokens\n",
      "Collected 2,401,422 tokens\n",
      "Collected 2,500,434 tokens\n",
      "Collected 2,600,557 tokens\n",
      "Collected 2,700,392 tokens\n",
      "Collected 2,801,133 tokens\n",
      "Collected 2,900,204 tokens\n",
      "Collected 3,001,947 tokens\n",
      "Collected 3,101,877 tokens\n",
      "Collected 3,200,123 tokens\n",
      "Collected 3,300,891 tokens\n",
      "Collected 3,400,373 tokens\n",
      "Collected 3,500,236 tokens\n",
      "Collected 3,600,082 tokens\n",
      "Collected 3,700,818 tokens\n",
      "Collected 3,802,260 tokens\n",
      "Collected 3,900,198 tokens\n",
      "Collected 4,000,067 tokens\n",
      "Collected 4,107,489 tokens\n",
      "Collected 4,201,785 tokens\n",
      "Collected 4,300,823 tokens\n",
      "Collected 4,408,609 tokens\n",
      "Collected 4,501,101 tokens\n",
      "Collected 4,600,302 tokens\n",
      "Collected 4,703,247 tokens\n",
      "Collected 4,800,100 tokens\n",
      "Collected 4,900,069 tokens\n",
      "Collected 5,002,422 tokens\n",
      "\n",
      "Final token count: 5,000,000\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage\n",
    "collected_tokens = []\n",
    "total_tokens = 0\n",
    "target_tokens = 5_000_000\n",
    "\n",
    "# Collect samples\n",
    "for sample in dataset:\n",
    "    # Get text from sample\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(text, truncation=False, padding=False)['input_ids']\n",
    "    \n",
    "    # Add to collection\n",
    "    collected_tokens.extend(tokens)\n",
    "    total_tokens += len(tokens)\n",
    "    \n",
    "    # Print progress every 100k tokens\n",
    "    if total_tokens // 100_000 > (total_tokens - len(tokens)) // 100_000:\n",
    "        print(f\"Collected {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Stop when we hit target\n",
    "    if total_tokens >= target_tokens:\n",
    "        break\n",
    "\n",
    "# Convert to numpy array and trim to exact size\n",
    "collected_tokens = np.array(collected_tokens[:target_tokens])\n",
    "print(f\"\\nFinal token count: {len(collected_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216b90ed-e11b-426d-965e-50c5b1a86e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([9765, 511])\n",
      "Target shape: torch.Size([9765, 511])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 512  # Changed from 1024\n",
    "n_sequences = len(collected_tokens) // sequence_length\n",
    "\n",
    "# Reshape tokens into sequences\n",
    "sequences = collected_tokens[:n_sequences * sequence_length].reshape(-1, sequence_length)\n",
    "\n",
    "# Create input and target pairs for causal language modeling\n",
    "input_sequences = sequences[:, :-1]  # all tokens except last\n",
    "target_sequences = sequences[:, 1:]  # all tokens except first\n",
    "\n",
    "# Convert to torch tensors\n",
    "inputs = torch.tensor(input_sequences)\n",
    "masks = torch.tensor(np.ones_like(input_sequences))\n",
    "targets = torch.tensor(target_sequences)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788a9148-64aa-40cb-ab49-a15b23411da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "val_idx = int(len(inputs) * (1 - val_split))\n",
    "\n",
    "# Split into train/val\n",
    "train_inputs = inputs[:val_idx]\n",
    "train_masks = masks[:val_idx]\n",
    "train_targets = targets[:val_idx]\n",
    "\n",
    "val_inputs = inputs[val_idx:]\n",
    "val_masks = masks[val_idx:]\n",
    "val_targets = targets[val_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LLMDataset(train_inputs, train_masks, train_targets)\n",
    "val_dataset = LLMDataset(val_inputs, val_masks, val_targets)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,  # Changed to 4\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0cad1cb-6c60-4d2a-bef2-cbb5c93129c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Training batches: 4394\n",
      "Validation batches: 489\n",
      "\n",
      "Batch shapes:\n",
      "input_ids: torch.Size([2, 511])\n",
      "attention_mask: torch.Size([2, 511])\n",
      "labels: torch.Size([2, 511])\n",
      "\n",
      "Sample decoded text:\n",
      ", is one that has a high level of viscosity\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Sample a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "# Decode a sample sequence\n",
    "sample_seq = sample_batch['input_ids'][0][:10].tolist()\n",
    "decoded = tokenizer.decode(sample_seq)\n",
    "print(f\"\\nSample decoded text:\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de1815f-6240-4884-b146-6d3b96fd1371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "#model = model.half()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5d6348-0c56-495f-bfbc-64e4f95a7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 0 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 2 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 2 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 4 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 4 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 6 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 6 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 8 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 8 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 10 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 10 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 12 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 12 - Input dtype: torch.float32, Output dtype: torch.float32\n",
      "Layer 14 - Input shape: torch.Size([1, 5, 2048]), Output shape: torch.Size([1, 5, 2048])\n",
      "Layer 14 - Input dtype: torch.float32, Output dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Initialize 8 Mamba blocks\n",
    "mamba_blocks = {}\n",
    "\n",
    "# Configuration for all blocks\n",
    "mamba_config = MambaConfig(\n",
    "    d_model=2048,          # Matches Llama's hidden size\n",
    "    n_layers=1             \n",
    ")\n",
    "\n",
    "# Create blocks for layers 0,2,4,6,8,10,12,14\n",
    "#for layer_idx in [1, 3, 5, 7, 8, 10, 12, 14]:\n",
    "for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "    \n",
    "    mamba_blocks[layer_idx] = Mamba(mamba_config).cuda()\n",
    "\n",
    "# Verify shapes for each block\n",
    "test_input = torch.randn(1, 5, 2048).cuda()  # Changed dtype\n",
    "for layer_idx, block in mamba_blocks.items():\n",
    "    test_output = block(test_input)\n",
    "    print(f\"Layer {layer_idx} - Input shape: {test_input.shape}, Output shape: {test_output.shape}\")\n",
    "    print(f\"Layer {layer_idx} - Input dtype: {test_input.dtype}, Output dtype: {test_output.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e08e220-c657-4d36-9735-84c305e33547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA trainable parameters: 0 (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Freeze LLaMA\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify LLaMA is frozen\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"LLaMA trainable parameters: {len(trainable_params)} (should be 0)\")\n",
    "\n",
    "# Ensure Mamba blocks are trainable\n",
    "for mamba in mamba_blocks.values():\n",
    "    for param in mamba.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d68dae-6a29-4d39-95c2-f14c380e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {}\n",
    "for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "    optimizers[layer_idx] = torch.optim.AdamW(mamba_blocks[layer_idx].parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14675a87-0ca2-4b56-8d37-c1dd9e4b1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40a65203-1f95-4f16-bda3-5470e8ac2675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64706dd4b158447e9bccaac20535ac51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/4394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizers[layer_idx]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Forward through Mamba\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m mamba_output \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute loss for this layer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m layer_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(mamba_output, attention_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:83\u001b[0m, in \u001b[0;36mMamba.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# x : (B, L, D)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# y : (B, L, D)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 83\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:111\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# x : (B, L, D)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# output : (B, L, D)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:222\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, L, ED)\u001b[39;00m\n\u001b[1;32m    221\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(x)\n\u001b[0;32m--> 222\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[1;32m    225\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(y) \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:268\u001b[0m, in \u001b[0;36mMambaBlock.ssm\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m    265\u001b[0m delta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(delta \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_proj\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpscan:\n\u001b[0;32m--> 268\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselective_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselective_scan_seq(x, delta, A, B, C, D)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/mamba.py:289\u001b[0m, in \u001b[0;36mMambaBlock.selective_scan\u001b[0;34m(self, x, delta, A, B, C, D)\u001b[0m\n\u001b[1;32m    285\u001b[0m deltaB \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m B\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[1;32m    287\u001b[0m BX \u001b[38;5;241m=\u001b[39m deltaB \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m hs \u001b[38;5;241m=\u001b[39m \u001b[43mpscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeltaA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m y \u001b[38;5;241m=\u001b[39m (hs \u001b[38;5;241m@\u001b[39m C\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\u001b[39;00m\n\u001b[1;32m    293\u001b[0m y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m D \u001b[38;5;241m*\u001b[39m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/pscan.py:181\u001b[0m, in \u001b[0;36mPScan.forward\u001b[0;34m(ctx, A_in, X_in)\u001b[0m\n\u001b[1;32m    178\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, D, npo2(L), N)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# parallel scan (modifies X in-place)\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mPScan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(A_in, X)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# slice [:, :L] (cut if there was padding)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mambapy/pscan.py:60\u001b[0m, in \u001b[0;36mPScan.pscan\u001b[0;34m(A, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m Xa \u001b[38;5;241m=\u001b[39m Xa\u001b[38;5;241m.\u001b[39mview(B, D, T\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m Xa[:, :, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39madd_(Aa[:, :, :, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmul(Xa[:, :, :, \u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 60\u001b[0m \u001b[43mAa\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAa\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m Aa \u001b[38;5;241m=\u001b[39m Aa[:, :, :, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     63\u001b[0m Xa \u001b[38;5;241m=\u001b[39m Xa[:, :, :, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "accumulation_steps = 32  # Define accumulation steps\n",
    "num_epochs = 1\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Track loss for each layer separately\n",
    "    layer_losses = {idx: 0.0 for idx in [0, 2, 4, 6, 8, 10, 12, 14]}\n",
    "    \n",
    "    # Progress bar for this epoch\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                       desc=f'Epoch {epoch+1}', \n",
    "                       leave=True)\n",
    "    \n",
    "    for i, batch in progress_bar:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        \n",
    "        # Forward pass through LLaMA\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Train each Mamba block independently\n",
    "        for layer_idx in [0, 2, 4, 6, 8, 10, 12, 14]:\n",
    "            # Get input and target output for this layer\n",
    "            layer_input = hidden_states[layer_idx]\n",
    "            attention_output = hidden_states[layer_idx + 1]\n",
    "            \n",
    "            optimizers[layer_idx].zero_grad()\n",
    "            \n",
    "            # Forward through Mamba\n",
    "            mamba_output = mamba_blocks[layer_idx](layer_input)\n",
    "            \n",
    "            # Compute loss for this layer\n",
    "            layer_loss = F.mse_loss(mamba_output, attention_output)\n",
    "            layer_loss = layer_loss / accumulation_steps\n",
    "            \n",
    "            # Accumulate loss for reporting\n",
    "            layer_losses[layer_idx] += layer_loss.item()\n",
    "            \n",
    "            # Backward for this layer\n",
    "            layer_loss.backward()\n",
    "            \n",
    "            del mamba_output\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizers[layer_idx].step()\n",
    "                optimizers[layer_idx].zero_grad()\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del outputs, hidden_states\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update progress bar with losses after accumulation steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            avg_losses = {\n",
    "                layer_idx: layer_losses[layer_idx] / accumulation_steps \n",
    "                for layer_idx in layer_losses\n",
    "            }\n",
    "            \n",
    "            # Update progress bar description with layer losses\n",
    "            loss_str = \" \".join([f\"L{idx}: {loss:.4f}\" for idx, loss in avg_losses.items()])\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1} | {loss_str}\"\n",
    "            )\n",
    "            \n",
    "            # Reset loss accumulators\n",
    "            layer_losses = {idx: 0.0 for idx in [0, 2, 4, 6, 8, 10, 12, 14]}\n",
    "\n",
    "    # End of epoch timing\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"\\nEpoch {epoch+1} completed in: {timedelta(seconds=int(epoch_time))}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in: {timedelta(seconds=int(total_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75609ff1-105b-4cd5-8621-e81b0ce99f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
